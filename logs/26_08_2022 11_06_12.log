Namespace(activation='relu', epochs=1000, hid_feats=100, k=10, latent_feats=20, lr1=0.01, lr2=0.01, mod1_feat_size=100, mod2_feat_size=100, normalization='batch', num_decoder_layer=7, num_encoder_layer=7, num_gcn1_layer=7, num_gcn2_layer=7, num_mod1_layer=7, num_mod2_layer=7, out_mod1_feats=100, out_mod2_feats=100, patience=2)
JointGCNAutoEncoder(
  (gcn1): GCNlayer(
    (gcn): ModuleList(
      (0): GraphConv(in=100, out=100, normalization=both, activation=None)
      (1): GraphConv(in=100, out=100, normalization=both, activation=None)
      (2): GraphConv(in=100, out=100, normalization=both, activation=None)
      (3): GraphConv(in=100, out=100, normalization=both, activation=None)
      (4): GraphConv(in=100, out=100, normalization=both, activation=None)
      (5): GraphConv(in=100, out=100, normalization=both, activation=None)
      (6): GraphConv(in=100, out=100, normalization=both, activation=None)
    )
    (gcn_acts): ModuleList(
      (0): ReLU()
      (1): ReLU()
      (2): ReLU()
      (3): ReLU()
      (4): ReLU()
      (5): ReLU()
      (6): ReLU()
    )
    (gcn_norm): ModuleList()
  )
  (gcn2): GCNlayer(
    (gcn): ModuleList(
      (0): GraphConv(in=100, out=100, normalization=both, activation=None)
      (1): GraphConv(in=100, out=100, normalization=both, activation=None)
      (2): GraphConv(in=100, out=100, normalization=both, activation=None)
      (3): GraphConv(in=100, out=100, normalization=both, activation=None)
      (4): GraphConv(in=100, out=100, normalization=both, activation=None)
      (5): GraphConv(in=100, out=100, normalization=both, activation=None)
      (6): GraphConv(in=100, out=100, normalization=both, activation=None)
    )
    (gcn_acts): ModuleList(
      (0): ReLU()
      (1): ReLU()
      (2): ReLU()
      (3): ReLU()
      (4): ReLU()
      (5): ReLU()
      (6): ReLU()
    )
    (gcn_norm): ModuleList()
  )
  (hid_encoder): HiddenEncoder(
    (hid_encoder): ModuleList(
      (0): Linear(in_features=100, out_features=100, bias=True)
      (1): Linear(in_features=100, out_features=100, bias=True)
      (2): Linear(in_features=100, out_features=100, bias=True)
      (3): Linear(in_features=100, out_features=100, bias=True)
      (4): Linear(in_features=100, out_features=100, bias=True)
      (5): Linear(in_features=100, out_features=100, bias=True)
      (6): Linear(in_features=100, out_features=20, bias=True)
    )
    (encoder_acts): ModuleList(
      (0): ReLU()
      (1): ReLU()
      (2): ReLU()
      (3): ReLU()
      (4): ReLU()
      (5): ReLU()
      (6): ReLU()
    )
    (encoder_norm): ModuleList(
      (0): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (6): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (hid_decoder): HiddenDecoder(
    (hid_decoder): ModuleList(
      (0): Linear(in_features=20, out_features=100, bias=True)
      (1): Linear(in_features=100, out_features=100, bias=True)
      (2): Linear(in_features=100, out_features=100, bias=True)
      (3): Linear(in_features=100, out_features=100, bias=True)
      (4): Linear(in_features=100, out_features=100, bias=True)
      (5): Linear(in_features=100, out_features=100, bias=True)
      (6): Linear(in_features=100, out_features=100, bias=True)
    )
    (decoder_acts): ModuleList(
      (0): ReLU()
      (1): ReLU()
      (2): ReLU()
      (3): ReLU()
      (4): ReLU()
      (5): ReLU()
      (6): ReLU()
    )
    (decoder_norm): ModuleList(
      (0): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (6): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (mod1_decoder): ModalityDecoder(
    (mod_decoder): ModuleList(
      (0): Linear(in_features=100, out_features=100, bias=True)
      (1): Linear(in_features=100, out_features=100, bias=True)
      (2): Linear(in_features=100, out_features=100, bias=True)
      (3): Linear(in_features=100, out_features=100, bias=True)
      (4): Linear(in_features=100, out_features=100, bias=True)
      (5): Linear(in_features=100, out_features=100, bias=True)
      (6): Linear(in_features=100, out_features=100, bias=True)
    )
    (mod_acts): ModuleList(
      (0): ReLU()
      (1): ReLU()
      (2): ReLU()
      (3): ReLU()
      (4): ReLU()
      (5): ReLU()
      (6): ReLU()
    )
    (mod_norm): ModuleList(
      (0): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (6): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (mod2_decoder): ModalityDecoder(
    (mod_decoder): ModuleList(
      (0): Linear(in_features=100, out_features=100, bias=True)
      (1): Linear(in_features=100, out_features=100, bias=True)
      (2): Linear(in_features=100, out_features=100, bias=True)
      (3): Linear(in_features=100, out_features=100, bias=True)
      (4): Linear(in_features=100, out_features=100, bias=True)
      (5): Linear(in_features=100, out_features=100, bias=True)
      (6): Linear(in_features=100, out_features=100, bias=True)
    )
    (mod_acts): ModuleList(
      (0): ReLU()
      (1): ReLU()
      (2): ReLU()
      (3): ReLU()
      (4): ReLU()
      (5): ReLU()
      (6): ReLU()
    )
    (mod_norm): ModuleList(
      (0): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (6): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
)
epoch:  0
training loss 1:  1.6295101642608643
training loss 2:  39968288.0
validation loss 1:  1.4663842916488647
validation loss 2:  39881764.0
rmse ADT to GEX:  7.147247314453125
rmse GEX to ADT:  4740.69970703125
epoch:  1
training loss 1:  1.4353266954421997
training loss 2:  39966516.0
validation loss 1:  1.4327062368392944
validation loss 2:  39897956.0
rmse ADT to GEX:  7.147247314453125
rmse GEX to ADT:  4740.69970703125
epoch:  2
training loss 1:  1.2652446031570435
training loss 2:  39964912.0
validation loss 1:  1.3902065753936768
validation loss 2:  39899908.0
rmse ADT to GEX:  7.147247314453125
rmse GEX to ADT:  4740.69970703125
epoch:  3
training loss 1:  1.1005761623382568
training loss 2:  39963796.0
validation loss 1:  1.2806010246276855
validation loss 2:  39820116.0
rmse ADT to GEX:  7.147247314453125
rmse GEX to ADT:  4740.69970703125
epoch:  4
training loss 1:  0.9515093564987183
training loss 2:  39963172.0
validation loss 1:  1.2779960632324219
validation loss 2:  39750456.0
rmse ADT to GEX:  7.147247314453125
rmse GEX to ADT:  4740.69970703125
epoch:  5
training loss 1:  0.8472915887832642
training loss 2:  39962308.0
validation loss 1:  1.4684422016143799
validation loss 2:  39552652.0
rmse ADT to GEX:  7.147247314453125
rmse GEX to ADT:  4740.69970703125
epoch:  6
training loss 1:  0.7924101948738098
training loss 2:  39961192.0
validation loss 1:  1.6431338787078857
validation loss 2:  39448332.0
rmse ADT to GEX:  7.147247314453125
rmse GEX to ADT:  4740.69970703125
epoch:  7
training loss 1:  0.7453437447547913
training loss 2:  39960312.0
validation loss 1:  1.5514687299728394
validation loss 2:  39469700.0
rmse ADT to GEX:  7.147247314453125
rmse GEX to ADT:  4740.69970703125
epoch:  8
training loss 1:  0.7079939246177673
training loss 2:  39959112.0
validation loss 1:  1.4623663425445557
validation loss 2:  39968328.0
rmse ADT to GEX:  7.147247314453125
rmse GEX to ADT:  4740.69970703125
epoch:  9
training loss 1:  0.6752637624740601
training loss 2:  39957876.0
validation loss 1:  1.3206572532653809
validation loss 2:  39849232.0
rmse ADT to GEX:  7.147247314453125
rmse GEX to ADT:  4740.69970703125
epoch:  10
training loss 1:  0.6468069553375244
training loss 2:  39956488.0
validation loss 1:  1.072896122932434
validation loss 2:  39671716.0
rmse ADT to GEX:  7.147247314453125
rmse GEX to ADT:  4740.69970703125
epoch:  11
training loss 1:  0.6493330597877502
training loss 2:  39955152.0
validation loss 1:  0.9520835876464844
validation loss 2:  39526208.0
rmse ADT to GEX:  7.147247314453125
rmse GEX to ADT:  4740.69970703125
epoch:  12
training loss 1:  0.5990130305290222
training loss 2:  39953400.0
validation loss 1:  1.2305891513824463
validation loss 2:  39491280.0
rmse ADT to GEX:  7.147247314453125
rmse GEX to ADT:  4740.69970703125
epoch:  13
training loss 1:  0.5873399972915649
training loss 2:  39951876.0
validation loss 1:  1.5668578147888184
validation loss 2:  39509524.0
rmse ADT to GEX:  7.147247314453125
rmse GEX to ADT:  4740.69970703125
epoch:  14
training loss 1:  0.5631948709487915
training loss 2:  39950424.0
validation loss 1:  1.5936106443405151
validation loss 2:  39555924.0
rmse ADT to GEX:  7.147247314453125
rmse GEX to ADT:  4740.69970703125
epoch:  15
training loss 1:  0.5319894552230835
training loss 2:  39948488.0
validation loss 1:  1.306350588798523
validation loss 2:  39606628.0
rmse ADT to GEX:  7.147247314453125
rmse GEX to ADT:  4740.69970703125
epoch:  16
training loss 1:  0.5083049535751343
training loss 2:  39946784.0
validation loss 1:  1.1494026184082031
validation loss 2:  39642400.0
rmse ADT to GEX:  7.147247314453125
rmse GEX to ADT:  4740.69970703125
epoch:  17
training loss 1:  0.4983183741569519
training loss 2:  39945084.0
validation loss 1:  1.1859437227249146
validation loss 2:  39682112.0
rmse ADT to GEX:  7.147247314453125
rmse GEX to ADT:  4740.69970703125
epoch:  18
training loss 1:  0.4876016080379486
training loss 2:  39943216.0
validation loss 1:  1.3235769271850586
validation loss 2:  39711452.0
rmse ADT to GEX:  7.147247314453125
rmse GEX to ADT:  4740.69970703125
epoch:  19
training loss 1:  0.4708097577095032
training loss 2:  39942072.0
validation loss 1:  1.3331834077835083
validation loss 2:  39752192.0
rmse ADT to GEX:  7.147247314453125
rmse GEX to ADT:  4740.69970703125
epoch:  20
training loss 1:  0.45804649591445923
training loss 2:  39939916.0
validation loss 1:  1.402816653251648
validation loss 2:  39864204.0
rmse ADT to GEX:  7.147247314453125
rmse GEX to ADT:  4740.69970703125
epoch:  21
training loss 1:  0.45338913798332214
training loss 2:  39938164.0
validation loss 1:  1.3828078508377075
validation loss 2:  39741108.0
rmse ADT to GEX:  7.147247314453125
rmse GEX to ADT:  4740.69970703125
epoch:  22
training loss 1:  0.45572584867477417
training loss 2:  39936512.0
validation loss 1:  1.4210610389709473
validation loss 2:  39782236.0
rmse ADT to GEX:  7.147247314453125
rmse GEX to ADT:  4740.69970703125
epoch:  23
training loss 1:  0.43748441338539124
training loss 2:  39934320.0
validation loss 1:  1.6737545728683472
validation loss 2:  39796892.0
rmse ADT to GEX:  7.147247314453125
rmse GEX to ADT:  4740.69970703125
epoch:  24
training loss 1:  0.4287277162075043
training loss 2:  39932364.0
validation loss 1:  1.5677911043167114
validation loss 2:  39795132.0
rmse ADT to GEX:  7.147247314453125
rmse GEX to ADT:  4740.69970703125
epoch:  25
training loss 1:  0.4269390404224396
training loss 2:  39930284.0
validation loss 1:  1.9327216148376465
validation loss 2:  39980548.0
rmse ADT to GEX:  7.147247314453125
rmse GEX to ADT:  4740.69970703125
epoch:  26
training loss 1:  0.42244502902030945
training loss 2:  39928128.0
validation loss 1:  1.7662183046340942
validation loss 2:  39861560.0
rmse ADT to GEX:  7.147247314453125
rmse GEX to ADT:  4740.69970703125
epoch:  27
training loss 1:  0.40188053250312805
training loss 2:  39925996.0
validation loss 1:  1.5370779037475586
validation loss 2:  39880508.0
rmse ADT to GEX:  7.147247314453125
rmse GEX to ADT:  4740.69970703125
epoch:  28
training loss 1:  0.39833399653434753
training loss 2:  39923780.0
validation loss 1:  1.6359139680862427
validation loss 2:  39874412.0
rmse ADT to GEX:  7.147247314453125
rmse GEX to ADT:  4740.69970703125
epoch:  29
training loss 1:  0.4007202684879303
training loss 2:  39921528.0
validation loss 1:  1.6366571187973022
validation loss 2:  39887280.0
rmse ADT to GEX:  7.147247314453125
rmse GEX to ADT:  4740.69970703125
epoch:  30
training loss 1:  0.3898300230503082
training loss 2:  39919440.0
validation loss 1:  1.531436800956726
validation loss 2:  39871964.0
rmse ADT to GEX:  7.147247314453125
rmse GEX to ADT:  4740.69970703125
epoch:  31
training loss 1:  0.3787279427051544
training loss 2:  39916908.0
validation loss 1:  1.4402799606323242
validation loss 2:  39867456.0
rmse ADT to GEX:  7.147247314453125
rmse GEX to ADT:  4740.69970703125
epoch:  32
training loss 1:  0.3820393681526184
training loss 2:  39914608.0
validation loss 1:  1.4219253063201904
validation loss 2:  39865552.0
rmse ADT to GEX:  7.147247314453125
rmse GEX to ADT:  4740.69970703125
epoch:  33
training loss 1:  0.37628448009490967
training loss 2:  39912188.0
validation loss 1:  1.3604930639266968
validation loss 2:  39868016.0
rmse ADT to GEX:  7.147247314453125
rmse GEX to ADT:  4740.69970703125
epoch:  34
training loss 1:  0.3723819851875305
training loss 2:  39909712.0
validation loss 1:  1.2678840160369873
validation loss 2:  39858948.0
rmse ADT to GEX:  7.147247314453125
rmse GEX to ADT:  4740.69970703125
epoch:  35
training loss 1:  0.3702927231788635
training loss 2:  39907040.0
validation loss 1:  1.2101002931594849
validation loss 2:  39856240.0
rmse ADT to GEX:  7.147247314453125
rmse GEX to ADT:  4740.69970703125
epoch:  36
training loss 1:  0.3622046411037445
training loss 2:  39904364.0
validation loss 1:  1.1652154922485352
validation loss 2:  39867736.0
rmse ADT to GEX:  7.147247314453125
rmse GEX to ADT:  4740.69970703125
epoch:  37
training loss 1:  0.3679324984550476
training loss 2:  39901812.0
validation loss 1:  1.0251818895339966
validation loss 2:  39869188.0
rmse ADT to GEX:  7.147247314453125
rmse GEX to ADT:  4740.69970703125
epoch:  38
training loss 1:  0.3922310769557953
training loss 2:  39899312.0
validation loss 1:  0.8831554055213928
validation loss 2:  39879144.0
rmse ADT to GEX:  7.147247314453125
rmse GEX to ADT:  4740.69970703125
early stopping for mod1 trigger
